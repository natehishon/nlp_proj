{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of sarcasm4.ipynb","provenance":[{"file_id":"1DUHAD46gM4ryIpmuyVMyurXaseGsljbq","timestamp":1620321732726}],"collapsed_sections":[],"authorship_tag":"ABX9TyP1xic3aKEF5IuLxRXXqhJ2"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJzW-KwR1zb6","executionInfo":{"status":"ok","timestamp":1617205542815,"user_tz":300,"elapsed":2835,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"42a773ee-d86b-4094-98e5-6482b65d63b9"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ejRY38OfjIA8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617206592669,"user_tz":300,"elapsed":3301,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"82cf011d-d9cc-499d-c808-99a48db41a9c"},"source":["import pandas as pd\n","import csv\n","\n","import tensorflow as tf# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')\n","\n","import torch# If there's a GPU available...\n","if torch.cuda.is_available():        # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")    \n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n","    print('We will use the GPU:', torch.cuda.get_device_name(0))# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","\n","df = pd.read_csv(\"sample_data/train-balanced-sarcasm_edit.csv\", names=['label','comment','author','subreddit','score','ups','downs','date','created_utc','parent_comment'],quoting=csv.QUOTE_NONE)\n","\n","df[\"comment\"] = df[\"comment\"].astype(str)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1VQKw_Udo-YT","executionInfo":{"status":"ok","timestamp":1617206598012,"user_tz":300,"elapsed":724,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"9e37cd7f-be28-4c0a-9015-b90774dc48b2"},"source":["print('Number of training sentences: {:,}\\n'.format(df.shape[0]))# Display 10 random rows from the data.\n","df.sample(10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training sentences: 24,961\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>comment</th>\n","      <th>author</th>\n","      <th>subreddit</th>\n","      <th>score</th>\n","      <th>ups</th>\n","      <th>downs</th>\n","      <th>date</th>\n","      <th>created_utc</th>\n","      <th>parent_comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>14074</th>\n","      <td>0</td>\n","      <td>There's honestly too many to have only 1 mvp</td>\n","      <td>MrCMoney</td>\n","      <td>SquaredCircle</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2016-09</td>\n","      <td>2016-09-10 20:55:05</td>\n","      <td>\"Who has been your personal MVP of the CWC tou...</td>\n","    </tr>\n","    <tr>\n","      <th>10206</th>\n","      <td>0</td>\n","      <td>Knicks fans and Nets fans don't always get alo...</td>\n","      <td>pillowpanda1</td>\n","      <td>nba</td>\n","      <td>29</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>2016-09</td>\n","      <td>2016-09-30 05:32:12</td>\n","      <td>What do you have against Brooklyn lol?</td>\n","    </tr>\n","    <tr>\n","      <th>8965</th>\n","      <td>1</td>\n","      <td>\"The internet is my safe space</td>\n","      <td>I will not tolerate any of your microagressions\"</td>\n","      <td>ok_backbay</td>\n","      <td>WorldOfWarships</td>\n","      <td>-11</td>\n","      <td>-11</td>\n","      <td>0</td>\n","      <td>2016-09</td>\n","      <td>2016-09-07 15:10:01</td>\n","    </tr>\n","    <tr>\n","      <th>24829</th>\n","      <td>0</td>\n","      <td>\"Well</td>\n","      <td>it's not like he is going to use it anyway.\"</td>\n","      <td>Xtr0</td>\n","      <td>pcmasterrace</td>\n","      <td>2</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-11</td>\n","      <td>2016-11-04 21:29:56</td>\n","    </tr>\n","    <tr>\n","      <th>22670</th>\n","      <td>0</td>\n","      <td>At first glance I thought that was Matt Serra</td>\n","      <td>KelLuvsOrngSoda</td>\n","      <td>MMA</td>\n","      <td>13</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-10</td>\n","      <td>2016-10-22 14:10:25</td>\n","      <td>I'm seeing a pattern here</td>\n","    </tr>\n","    <tr>\n","      <th>21207</th>\n","      <td>1</td>\n","      <td>Damn censorship</td>\n","      <td>pdrocker1</td>\n","      <td>TopMindsOfReddit</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>0</td>\n","      <td>2016-09</td>\n","      <td>2016-09-21 20:09:52</td>\n","      <td>That's because the media in the UK isn't allow...</td>\n","    </tr>\n","    <tr>\n","      <th>21055</th>\n","      <td>1</td>\n","      <td>\"See</td>\n","      <td>you don't get it</td>\n","      <td>9/11 was an inside job</td>\n","      <td>not a terrorist attack.\"</td>\n","      <td>thecrusader54</td>\n","      <td>CringeAnarchy</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21897</th>\n","      <td>1</td>\n","      <td>\"As proved by my mom being taller than my dad</td>\n","      <td>men are not actually taller than women.\"</td>\n","      <td>climacus76</td>\n","      <td>LiverpoolFC</td>\n","      <td>19</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-10</td>\n","      <td>2016-10-06 09:51:37</td>\n","    </tr>\n","    <tr>\n","      <th>24711</th>\n","      <td>0</td>\n","      <td>It has been reduced to 17 turns (spawned in sh...</td>\n","      <td>Aoae</td>\n","      <td>cataclysmdda</td>\n","      <td>9</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-11</td>\n","      <td>2016-11-21 04:36:58</td>\n","      <td>Personal record for quickest death - 49 minute...</td>\n","    </tr>\n","    <tr>\n","      <th>10455</th>\n","      <td>1</td>\n","      <td>He just has to keep blowing up SpaceX rockets ...</td>\n","      <td>_badwithcomputer</td>\n","      <td>spacex</td>\n","      <td>13</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>2016-10</td>\n","      <td>2016-10-05 03:46:57</td>\n","      <td>This is a very strange statement coming from B...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      label  ...                                     parent_comment\n","14074     0  ...  \"Who has been your personal MVP of the CWC tou...\n","10206     0  ...             What do you have against Brooklyn lol?\n","8965      1  ...                                2016-09-07 15:10:01\n","24829     0  ...                                2016-11-04 21:29:56\n","22670     0  ...                          I'm seeing a pattern here\n","21207     1  ...  That's because the media in the UK isn't allow...\n","21055     1  ...                                                  0\n","21897     1  ...                                2016-10-06 09:51:37\n","24711     0  ...  Personal record for quickest death - 49 minute...\n","10455     1  ...  This is a very strange statement coming from B...\n","\n","[10 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"PdEc2_fnpjMn"},"source":["sentences = df.comment.values\n","labels = df.label.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPU9l83D2Ho9","executionInfo":{"status":"ok","timestamp":1617206603891,"user_tz":300,"elapsed":1840,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"fbf697b4-a1d6-470e-a6eb-36bda481a6dd"},"source":["from transformers import BertTokenizer# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mzw3Wd-52OI0","executionInfo":{"status":"ok","timestamp":1617206613953,"user_tz":300,"elapsed":7244,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"a942255b-0186-4c2a-daaa-c1f361f2d8bd"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []# For every sentence...\n","for sent in sentences:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'                        \n","                        # This function also supports truncation and conversion\n","                        # to pytorch tensors, but we need to do padding, so we\n","                        # can't use these features :( .\n","                        #max_length = 128,          # Truncate all sentences.\n","                        #return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_sent)# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[1])\n","print('Token IDs:', input_ids[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  NC and NH.\n","Token IDs: [101, 13316, 1998, 18699, 1012, 102]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQlmu7cI4sns","executionInfo":{"status":"ok","timestamp":1617206618542,"user_tz":300,"elapsed":469,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"dc05eb72-3ed3-4a63-8b34-05dfadc7ae27"},"source":["print('Max sentence length: ', max([len(sen) for sen in input_ids]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Max sentence length:  158\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RifRLO1R48Ku"},"source":["# We'll borrow the `pad_sequences` utility function to do this.\n","from keras.preprocessing.sequence import pad_sequences# Set the maximum sequence length.\n","# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n","# maximum training sentence length of 47...\n","MAX_LEN = 160\n","\n","\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YS9Tyb5w5PEI"},"source":["# Create attention masks\n","attention_masks = []# For each sentence...\n","for sent in input_ids:\n","    \n","    # Create the attention mask.\n","    #   - If a token ID is 0, then it's padding, set the mask to 0.\n","    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CynIOVXF5lDD"},"source":["# Use train_test_split to split our data into train and validation sets for\n","# training\n","from sklearn.model_selection import train_test_split# Use 90% for training and 10% for validation.\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                                            random_state=2018, test_size=0.1)\n","# Do the same for the masks.\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n","                                             random_state=2018, test_size=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Paa0hFc5swW","executionInfo":{"status":"ok","timestamp":1617206704600,"user_tz":300,"elapsed":372,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"dac712e4-43ab-4ec9-eff5-fa143154f941"},"source":["# Convert all inputs and labels into torch tensors, the required datatype \n","# for our model.\n","\n","import torch\n","import numpy as np\n","\n","\n","\n","\n","\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  from ipykernel import kernelapp as app\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  app.launch_new_instance()\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"r1nMj-z98nVV"},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler# The DataLoader needs to know our batch size for training, so we specify it \n","# here.\n","# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n","# 16 or 32.\n","batch_size = 32# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUFPkege81aC","executionInfo":{"status":"ok","timestamp":1617206716733,"user_tz":300,"elapsed":4255,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"78d7be36-1f31-44c7-c536-1535cc88fc7b"},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OpWzbN4qAIuG","executionInfo":{"status":"ok","timestamp":1617206722835,"user_tz":300,"elapsed":718,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"e786e43f-e53e-4037-df77-a8ec05174e23"},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","print('==== Embedding Layer ====\\n') \n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","    print('\\n==== First Transformer ====\\n')\n","    for p in params[5:21]:\n","      print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","      print('\\n==== Output Layer ====\\n')\n","    for p in params[-4:]:\n","      print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","\n","==== Output Layer ====\n","\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wDQCebyBAlm2"},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","from transformers import get_linear_schedule_with_warmup# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkb7kWAGAqD3"},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q742nD1uAuHr"},"source":["import time\n","import datetime\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gMhnghW7A1Ex","executionInfo":{"status":"ok","timestamp":1617208176871,"user_tz":300,"elapsed":1440317,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"7d30d8e5-fdd4-405f-dd22-55a4326c284a"},"source":["import random# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)# Store the average loss after each epoch so we can plot them.\n","loss_values = []# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')    # Measure how long the training epoch takes.\n","    t0 = time.time()    # Reset the total loss for this epoch.\n","    total_loss = 0    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()                # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()        # Perform a backward pass to calculate the gradients.\n","        loss.backward()        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()        # Update the learning rate.\n","        scheduler.step()    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)    \n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.    print(\"\")\n","    print(\"Running Validation...\")    \n","    t0 = time.time()    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():                    # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy        # Track the number of batches\n","        nb_eval_steps += 1    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","    print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    702.    Elapsed: 0:00:20.\n","  Batch    80  of    702.    Elapsed: 0:00:40.\n","  Batch   120  of    702.    Elapsed: 0:00:59.\n","  Batch   160  of    702.    Elapsed: 0:01:19.\n","  Batch   200  of    702.    Elapsed: 0:01:39.\n","  Batch   240  of    702.    Elapsed: 0:01:59.\n","  Batch   280  of    702.    Elapsed: 0:02:19.\n","  Batch   320  of    702.    Elapsed: 0:02:38.\n","  Batch   360  of    702.    Elapsed: 0:02:58.\n","  Batch   400  of    702.    Elapsed: 0:03:18.\n","  Batch   440  of    702.    Elapsed: 0:03:38.\n","  Batch   480  of    702.    Elapsed: 0:03:57.\n","  Batch   520  of    702.    Elapsed: 0:04:17.\n","  Batch   560  of    702.    Elapsed: 0:04:37.\n","  Batch   600  of    702.    Elapsed: 0:04:57.\n","  Batch   640  of    702.    Elapsed: 0:05:17.\n","  Batch   680  of    702.    Elapsed: 0:05:36.\n","\n","  Average training loss: 0.60\n","  Training epcoh took: 0:05:47\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation took: 0:00:13\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    702.    Elapsed: 0:00:20.\n","  Batch    80  of    702.    Elapsed: 0:00:40.\n","  Batch   120  of    702.    Elapsed: 0:00:59.\n","  Batch   160  of    702.    Elapsed: 0:01:19.\n","  Batch   200  of    702.    Elapsed: 0:01:39.\n","  Batch   240  of    702.    Elapsed: 0:01:59.\n","  Batch   280  of    702.    Elapsed: 0:02:18.\n","  Batch   320  of    702.    Elapsed: 0:02:38.\n","  Batch   360  of    702.    Elapsed: 0:02:58.\n","  Batch   400  of    702.    Elapsed: 0:03:18.\n","  Batch   440  of    702.    Elapsed: 0:03:38.\n","  Batch   480  of    702.    Elapsed: 0:03:57.\n","  Batch   520  of    702.    Elapsed: 0:04:17.\n","  Batch   560  of    702.    Elapsed: 0:04:37.\n","  Batch   600  of    702.    Elapsed: 0:04:57.\n","  Batch   640  of    702.    Elapsed: 0:05:16.\n","  Batch   680  of    702.    Elapsed: 0:05:36.\n","\n","  Average training loss: 0.50\n","  Training epcoh took: 0:05:47\n","Running Validation...\n","  Accuracy: 0.71\n","  Validation took: 0:00:13\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    702.    Elapsed: 0:00:20.\n","  Batch    80  of    702.    Elapsed: 0:00:40.\n","  Batch   120  of    702.    Elapsed: 0:00:59.\n","  Batch   160  of    702.    Elapsed: 0:01:19.\n","  Batch   200  of    702.    Elapsed: 0:01:39.\n","  Batch   240  of    702.    Elapsed: 0:01:59.\n","  Batch   280  of    702.    Elapsed: 0:02:19.\n","  Batch   320  of    702.    Elapsed: 0:02:38.\n","  Batch   360  of    702.    Elapsed: 0:02:58.\n","  Batch   400  of    702.    Elapsed: 0:03:18.\n","  Batch   440  of    702.    Elapsed: 0:03:38.\n","  Batch   480  of    702.    Elapsed: 0:03:57.\n","  Batch   520  of    702.    Elapsed: 0:04:17.\n","  Batch   560  of    702.    Elapsed: 0:04:37.\n","  Batch   600  of    702.    Elapsed: 0:04:57.\n","  Batch   640  of    702.    Elapsed: 0:05:17.\n","  Batch   680  of    702.    Elapsed: 0:05:36.\n","\n","  Average training loss: 0.38\n","  Training epcoh took: 0:05:47\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation took: 0:00:13\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    702.    Elapsed: 0:00:20.\n","  Batch    80  of    702.    Elapsed: 0:00:40.\n","  Batch   120  of    702.    Elapsed: 0:00:59.\n","  Batch   160  of    702.    Elapsed: 0:01:19.\n","  Batch   200  of    702.    Elapsed: 0:01:39.\n","  Batch   240  of    702.    Elapsed: 0:01:59.\n","  Batch   280  of    702.    Elapsed: 0:02:18.\n","  Batch   320  of    702.    Elapsed: 0:02:38.\n","  Batch   360  of    702.    Elapsed: 0:02:58.\n","  Batch   400  of    702.    Elapsed: 0:03:18.\n","  Batch   440  of    702.    Elapsed: 0:03:38.\n","  Batch   480  of    702.    Elapsed: 0:03:57.\n","  Batch   520  of    702.    Elapsed: 0:04:17.\n","  Batch   560  of    702.    Elapsed: 0:04:37.\n","  Batch   600  of    702.    Elapsed: 0:04:57.\n","  Batch   640  of    702.    Elapsed: 0:05:17.\n","  Batch   680  of    702.    Elapsed: 0:05:36.\n","\n","  Average training loss: 0.29\n","  Training epcoh took: 0:05:47\n","Running Validation...\n","  Accuracy: 0.69\n","  Validation took: 0:00:13\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"njTzUWaVH_lp","executionInfo":{"status":"ok","timestamp":1617208298891,"user_tz":300,"elapsed":2355,"user":{"displayName":"Nate Hishon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhiHiwAROxR7xE7LcPpZYEfIezQLLHVy-xsDM0lBw=s64","userId":"10072247869868623848"}},"outputId":"91f8c722-bc01-418b-abc4-b71090d44448"},"source":["import plotly.express as px\n","f = pd.DataFrame(loss_values)\n","f.columns=['Loss']\n","fig = px.line(f, x=f.index, y=f.Loss)\n","fig.update_layout(title='Training loss of the Model',\n","                   xaxis_title='Epoch',\n","                   yaxis_title='Loss')\n","fig.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"d8a80be7-1bad-4686-a26b-0d7b878ce59d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"d8a80be7-1bad-4686-a26b-0d7b878ce59d\")) {\n","                    Plotly.newPlot(\n","                        'd8a80be7-1bad-4686-a26b-0d7b878ce59d',\n","                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"index=%{x}<br>Loss=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0, 1, 2, 3], \"xaxis\": \"x\", \"y\": [0.6045121411312679, 0.4983067918579463, 0.3819863171956138, 0.29191475461351224], \"yaxis\": \"y\"}],\n","                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Training loss of the Model\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Loss\"}}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('d8a80be7-1bad-4686-a26b-0d7b878ce59d');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{"tags":[]}}]}]}